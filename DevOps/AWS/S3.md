#infinitely-scaling-storage

#### Use Cases
- Backup & Storage
- Disaster Recovery
- Archive
- Hybrid Cloud storage
- Application hosting
- Media hosting
- Data lakes & big data analytics
- Software delivery
- Static website



### Buckets
- file storage container / directory `buckets`
- Must have globally unique name `across all regions & all accounts`
- Looks like global, but associated with a region
- Naming :
	- No UP & No underscore
	- 3-63 char long
	- Not an IP
	- Must start with LP & num
	- Not prefix with `xn--`
	- Not suffix with `-s3alias`
### Objects
- Files in a bucket
- `Key` is full path  `s3://my-bucket/my_file.txt`
- Max size `5TB`, also files should be `5GB` split as multi-part upload


### Security
- User-Based
	- IAM Policies
- Resource-Based
	- Bucket Policies `from s3 console itself`
	- Sample policy 
	 ```json {
    "Version": "2012-10-17",
    "Id": "Policy1729053362133",
    "Statement": [
        {
            "Sid": "Stmt1729053360245",
            "Effect": "Allow",
            "Principal": "*",
            "Action": "s3:GetObject",
            "Resource": "arn:aws:s3:::sugabucket/*"
        }
    ]
} ```
	- Object Access Control List (ACL) - finer grain
	- Bucket Access Control List - less common
- Encryption



### Versioning
- Bucket Level
- Protects against unintended deletes
- easy roll back to previous version


--- 
### Replication
- Must enable versioning
- CRR `Cross Region Replication` ==Low latency==
- SRR `Same Region Replication` ==log aggregation==
- Buckets can be in different AWS accounts

> after enabling replication only new objects are replicated, for older ones need to do ==S3 Batch Replication==

#### Steps to Replication
- Have a source & destination with versioning enabled
- `Under source bucket -> Management -> create replica policy`
- if old files exists, add a batch job
> replicated files will have the same version ID

> By default ==Delete marker== is disabled, we have to force enable
---
### Storage Class
- Standard
	- Frequent data
	- Low latency & high throughput
	- sustain 2 concurrent facility failures
	
- Standard IA
	- lower cost than standard
	- usecase: `Disaster Recovery & backups`
	 - Can be done with single AZ ( for high availablity )
 - Glacier
	- Low-cost
	- Pay for size & object retrieval
	- Types:
		- Glacier Instant Retrieval
			- Minimum storage duration of 90 days
		- Glacier Flexible Retrieval
			- flexible timings
				- 1 to 5 mns, 3 to 5 hrs & 5 to 12 hrs
		- Glacier Deep Archive
			 - minimum storage duration of 180 days & retrival time of 12 hrs (standard ) & 48 hours for bulk
- Intelligent Tiering
	- Small monthly monitoring and auto-tiering fee
	- moves objects automatically between tiers based on usage
	- No fee 
	- Frequent Access tier ( automatic ) : default one
	- Infrequent Access Tier : objects not accessed for 30 days

> Can move between classes manually or using s3 lifecycle configurations


### Lifecycle Rules
- Transition Actions
- Expiration Actions
> can be specified of specific tag & path


---
### Requester Pays
- In general owner pay for all s3 storage
- we can make user (requester ) to pay networking cost for downloads
![[Pasted image 20241016152707.png]]

### Event Notifications
- like `S3:ObjectCreated, S3:ObjectRemoved, S3:objectRestore`


### S3- Baseline Performance

- Multi - Paret upload
	- divide as parts, and each parts will be done as parallel uploads
	![[Pasted image 20241016172637.png]]
- S3 transfer Acceleration
	- increase transfer speed by transferring to edge then will be forwarded to s3
	![[Pasted image 20241016172729.png]]


### S3 Batch Operations
- For performing bulk operations on existing s3 objects
	- Includes: 
		- `Modify object metadata & properties`
		- `Copy objects between s3 buckets`
		- `Encrypt un-encrypted objects`
		- `Modify ACLs, & tags`
		- `Restore objects from S3 Gacier`
		- `Invoke Lambda function to perform custom action on each object`



### S3 Storage Lens
- Analyze & optimize storage across entire org
- Can be configured to export metric